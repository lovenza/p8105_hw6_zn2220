---
title: "p8105_hw6_zn2220"
author: "Ziang Niu"
date: "2025-11-25"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(janitor)
library(broom)
library(p8105.datasets)
library(modelr)
```

# Problem 1

## Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved.

Binary variable: 1 if Closed by arrest, 0 otherwise

```{r}
data_url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"

homicides_raw <- read_csv(data_url) 

homicides <- homicides_raw %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    solved = disposition == "Closed by arrest"
  )
```

## Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake.

```{r}
homicides <- homicides %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))
```

## Limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

```{r}
homicides <- homicides %>% 
  filter(
    victim_race %in% c("Black", "White"),
    !victim_age %in% c("Unknown"),
    !victim_sex %in% c("Unknown")
    ) %>% 
  mutate(
    victim_age = as.numeric(victim_age) 
  )
```

## For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
baltimore_data <- homicides %>% 
  filter(city_state == "Baltimore, MD")

fit <- glm(solved ~ victim_age + victim_sex + victim_race, 
           data = baltimore_data, 
           family = binomial)

balitmore_glm_results <- broom::tidy(fit, conf.int = TRUE, exponentiate = TRUE)

balitmore_gender_results <- balitmore_glm_results %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, conf.low, conf.high)

print(balitmore_glm_results)
print(balitmore_gender_results)
```

Holding victim age and race constant, the odds of resolving a homicide case for a male victim are approximately 57.4% (1 - 0.426) lower than for a female victim in Baltimore.

## Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
city_glm_results <- homicides %>%
  nest(data = -city_state) %>% 
  mutate(
    fit = map(data, function(df) {
      glm(solved ~ victim_sex + victim_age + victim_race, 
          data = df, 
          family = binomial)
    }),
    tidied = map(fit, ~tidy(., conf.int = TRUE, exponentiate = TRUE))
  ) %>% 
  unnest(tidied) %>% 
  select(-data, -fit)

city_gender_results <- city_glm_results %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, estimate, conf.low, conf.high)

summary(city_glm_results)
knitr::kable(city_gender_results)
```

In the vast majority of these cities, the estimated odds ratio is below 1.0, indicating that homicides involving male victims are less likely to be solved than those involving female victims.

For many major cities (such as New York, Chicago, and Baltimore), this disparity is statistically significant because the entire confidence interval falls below 1.0. While a few cities (like Albuquerque and Stockton) show an estimate greater than 1.0, their confidence intervals are very wide and include the number 1, meaning there is no statistical evidence that male cases are solved at a higher rate in any of these cities.

## Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r}
city_gender_results %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(
    title = "Odds Ratios for Solving Homicides (Male vs Female)",
    x = "City",
    y = "Odds Ratio"
  ) +
  theme_minimal()
```

This forest plot reveals a consistent systemic trend across major U.S. cities where homicides involving male victims are significantly less likely to be solved than those involving female victims, as indicated by the vast majority of odds ratios falling to the left of the red reference line (OR < 1). Cities with narrower confidence intervals, such as New York, Chicago, and Baltimore, show a statistically significant disparity, confirming that the lower clearance rate for males in these locations is not due to random chance. Conversely, the cities at the top of the chart, like Albuquerque and Stockton, display extremely wide error bars that cross the threshold of 1, reflecting high uncertainty or smaller sample sizes rather than a definitive absence of bias.

# Problem 2

## Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities.

```{r}
data("weather_df")

weather_data = 
  weather_df %>% 
  select(tmax, tmin, prcp) %>% 
  drop_na()

boot_straps = 
  weather_data %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin + prcp, data = .x)),
    results_glance = map(models, glance),
    results_tidy = map(models, tidy)
  )

boot_results = 
  boot_straps %>% 
  select(.id, results_glance) %>% 
  unnest(results_glance) %>% 
  select(.id, r.squared) %>% 
  left_join(
    boot_straps %>% 
      select(.id, results_tidy) %>% 
      unnest(results_tidy) %>% 
      select(.id, term, estimate) %>% 
      pivot_wider(names_from = term, values_from = estimate) %>% 
      mutate(
        beta_prod = tmin * prcp
      ),
    by = ".id"
  ) %>% 
  select(.id, r.squared, beta_prod) %>% 
  pivot_longer(
    cols = c(r.squared, beta_prod),
    names_to = "term",
    values_to = "estimate"
  ) %>% 
  drop_na()

print(boot_results)
```

## Plot the distribution of your estimates, and describe these in words.

```{r}
boot_results %>% 
  ggplot(aes(x = estimate)) +
  geom_density(fill = "blue", alpha = 0.5) +
  facet_wrap(~term, scales = "free") +
  labs(
    title = "Bootstrap Distributions of Estimates",
    subtitle = "Based on 5000 bootstrap samples",
    x = "Estimate Value",
    y = "Density"
  ) +
  theme_minimal()
```

Left Panel: $$\beta_{\text{prod}} = \hat{\beta}_1 \times \hat{\beta}_2$$

The bootstrap distribution of $\beta_{\text{prod}}$ is approximately unimodal with a slight left skew and is centered around –0.006. Most estimates fall between –0.008 and –0.004, and the entire distribution is strictly negative. This indicates that the two predictors have effects in opposite directions. Because minimum temperature typically has a positive relationship with maximum temperature, the consistently negative product implies that precipitation has a stable negative coefficient across the bootstrap samples.

Right Panel: $$r^2$$

The bootstrap distribution of $r^2$ is highly concentrated, peaking around 0.940–0.942, with a left tail extending to roughly 0.930 and an upper cutoff near 0.950. These consistently high values demonstrate that the model explains about 93%–95% of the variation in maximum temperature, indicating a strong and robust goodness-of-fit.

## Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for $\hat{r}^2$ and $\hat{\beta}_1 / \hat{\beta}_2$.

```{r}
ci_results = 
  boot_results %>% 
  group_by(term) %>% 
  summarize(
    ci_lower = quantile(estimate, 0.025),
    ci_upper = quantile(estimate, 0.975)
  )

knitr::kable(ci_results)
```

+ beta_prod (Significant Negative Effect):  
The 95% confidence interval for $\beta_{\text{prod}} = \hat{\beta}_1 \times \hat{\beta}_2$ falls between –0.0082 and –0.0037. Because this entire interval is strictly negative and does not include zero, we conclude with 95% confidence that the product of the coefficients is statistically significant and negative. This indicates that the two predictors consistently act in opposite directions or interact negatively in affecting maximum temperature.

+ r.squared (High Predictive Power):  
The 95% confidence interval for $r^2$ ranges from 0.934 to 0.947. This narrow and high interval shows that the model is highly robust. We can be 95% confident that the predictors explain between 93.4% and 94.7% of the variation in maximum temperature, demonstrating strong and stable predictive performance.

# Problem 3

## Load and clean the data for regression analysis

```{r}
birthweight_df = 
  read_csv("https://p8105.com/data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex = as.factor(babysex),
    frace = as.factor(frace),
    mrace = as.factor(mrace),
    malform = as.factor(malform)
  ) |> 
  drop_na()
```

## Propose a regression model for birthweight. Describe your modeling process and show a plot of model residuals against fitted values

```{r}
my_model = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + smoken + wtgain, data = birthweight_df)

birthweight_df |> 
  add_predictions(my_model) |> 
  add_residuals(my_model) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values (Prediction)",
    y = "Residuals"
  )
```

The plot of model residuals against fitted values reveals that for the majority of the data (central mass), the residuals are clustered around zero, indicating that the model fits reasonably well for babies with average birthweights. However, there is some evidence of non-linearity or uncaptured variance at lower predicted birthweights, where residuals tend to be positive. This suggests the model may slightly underpredict the weight of smaller babies or that other factors are influencing low birthweight outcomes.

## Compare your model to two others & Make this comparison in terms of the cross-validated prediction error

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

cv_results = 
  cv_df |> 
  mutate(
    proposed_mod  = map(train, \(df) lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + smoken + wtgain, data = df)),
    main_eff_mod  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    interaction_mod = map(train, \(df) lm(bwt ~ bhead * blength * babysex, data = df))
  ) |> 
  mutate(
    rmse_proposed = map2_dbl(proposed_mod, test, \(mod, df) rmse(mod, df)),
    rmse_main_eff = map2_dbl(main_eff_mod, test, \(mod, df) rmse(mod, df)),
    rmse_interaction = map2_dbl(interaction_mod, test, \(mod, df) rmse(mod, df))
  )

cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

Comparing the proposed model against two others using 100-fold cross-validation. The violin plot of Root Mean Squared Errors (RMSE) shows:

+ Main Effects Model (Length + Gestational Age): This model has the highest RMSE (distribution centered above 325), indicating it has the poorest predictive performance among the three. * Interaction Model (Head + Length + Sex interactions): This model performs significantly better than the main effects model, with a lower RMSE distribution .

+ Proposed Model: My proposed model, which includes a broader range of predictors (including mother's weight gain, smoking habits, etc.), achieved the lowest RMSE (centered around 275). This suggests that incorporating maternal health and behavioral factors alongside infant biometrics provides the most accurate prediction of birthweight in this dataset.